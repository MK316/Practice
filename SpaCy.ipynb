{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJ0IgNDEAgHaIyPABQ9VXB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkyuhufs/Practice/blob/main/SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SpaCy practice\n",
        "\n",
        "https://colab.research.google.com/github/DerwenAI/spaCy_tuTorial/blob/master/spaCy_tuTorial.ipynb#scrollTo=qRCCr_LmW_1-"
      ],
      "metadata": {
        "id": "pgIFBfJTXE3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy 불러오고, model 설치 (model은 sm, medium, large가능)"
      ],
      "metadata": {
        "id": "N7ONa8ZuXRz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qRCCr_LmW_1-"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 생성 및 텍스트의 간단 nlp를 doc변수에 저장"
      ],
      "metadata": {
        "id": "-OXslMhnX3kl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGRPv_S3W_2B"
      },
      "outputs": [],
      "source": [
        "text = \"The rain in Spain falls mainly on the plain.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc변수를 padas이용 data frame 변환"
      ],
      "metadata": {
        "id": "xTlrBkd9YOPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da0qrqlaW_2D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "    \n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "displacy이용해서 문장구조 시각화"
      ],
      "metadata": {
        "id": "hCgRrTkQZAj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX8mAAH6W_2G"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 문장이상일때는? There are features for sentence boundary detection (SBD) – also known as sentence segmentation – based on the builtin/default sentencizer:"
      ],
      "metadata": {
        "id": "b18oeogSZdTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuSSTFr1W_2H"
      },
      "outputs": [],
      "source": [
        "text2 = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in. Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
        "\n",
        "doc2 = nlp(text2)\n",
        "\n",
        "for sent in doc2.sents:\n",
        "    print(\">\", sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장안의 단어갯수 (공백포함)"
      ],
      "metadata": {
        "id": "wtdHEoWjaOQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZbmn7oKW_2H"
      },
      "outputs": [],
      "source": [
        "for sent in doc2.sents:\n",
        "    print(\">\", sent.start, sent.end)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한문장만 뽑을때 인덱스이용"
      ],
      "metadata": {
        "id": "wib73maVab-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pSc2wVqW_2I"
      },
      "outputs": [],
      "source": [
        "doc2[48:54]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language Understanding\n",
        "\n",
        "Now let's dive into some of the spaCy features for NLU. Given that we have a parse of a document, from a purely grammatical standpoint we can pull the noun chunks, i.e., each of the noun phrases:"
      ],
      "metadata": {
        "id": "GwCplYrTbENP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5OdqPafW_2O"
      },
      "outputs": [],
      "source": [
        "text3 = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
        "doc3 = nlp(text3)\n",
        "\n",
        "for chunk in doc3.noun_chunks:\n",
        "    print(chunk.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " identify named entities within the text, i.e., the proper nouns:"
      ],
      "metadata": {
        "id": "pz7uQF03bvzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILlnOP98W_2P"
      },
      "outputs": [],
      "source": [
        "for ent in doc3.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entities 시각화"
      ],
      "metadata": {
        "id": "nVU3cZuhcAbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDJi3OzkW_2P"
      },
      "outputs": [],
      "source": [
        "displacy.render(doc3, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a spaCy integration for WordNet called spacy-wordnet by Daniel Vila Suero\n",
        "일단 nltk에서 wordnet불러옴"
      ],
      "metadata": {
        "id": "_I1elxCkcgIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeRKwjdlW_2Q"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline이용\n",
        "we'll add the WordnetAnnotator from the spacy-wordnet project"
      ],
      "metadata": {
        "id": "LWPv98Y9c4JK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMrPjEYKW_2Q"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et-d312YW_2R"
      },
      "outputs": [],
      "source": [
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "print(\"before\", nlp.pipe_names)\n",
        "\n",
        "if \"WordnetAnnotator\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n",
        "    \n",
        "print(\"after\", nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어단어는 다의어 (예, withdraw)"
      ],
      "metadata": {
        "id": "MA3QMmV1dek0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOk4QWLMW_2R"
      },
      "outputs": [],
      "source": [
        "token = nlp(\"withdraw\")[0]\n",
        "token._.wordnet.synsets()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "withdraw의 다의어와 관련된 의미영역 추출\n",
        "(궁극적으로는 시각화가능; 아직 어려운 영역)"
      ],
      "metadata": {
        "id": "M-IVOJVceRdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VELj22x9W_2S"
      },
      "outputs": [],
      "source": [
        "token._.wordnet.wordnet_domains()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러나 다른 방법으로 우리가 이미 withdraw가 사용되는 문맥을 지정할 수 있음 (예, finace, banking)"
      ],
      "metadata": {
        "id": "LwIV49RTekVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IcK6npTW_2S"
      },
      "outputs": [],
      "source": [
        "domains = [\"finance\", \"banking\"]\n",
        "sentence = nlp(\"I want to withdraw 5,000 euros.\")\n",
        "\n",
        "enriched_sent = []\n",
        "\n",
        "for token in sentence:\n",
        "    # get synsets within the desired domains\n",
        "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
        "    \n",
        "    if synsets:\n",
        "        lemmas_for_synset = []\n",
        "        \n",
        "        for s in synsets:\n",
        "            # get synset variants and add to the enriched sentence\n",
        "            lemmas_for_synset.extend(s.lemma_names())\n",
        "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n",
        "    else:\n",
        "        enriched_sent.append(token.text)\n",
        "\n",
        "print(\" \".join(enriched_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "시각화예시\n",
        "Let's analyze text data from the party conventions during the 2012 US Presidential elections. It may take a minute or two to run, but the results from all that number crunching is worth the wait.\n",
        "\n",
        "(an interactive visualization for understanding texts: scattertext, a product of the genius of Jason Kessler.)"
      ],
      "metadata": {
        "id": "fLK72QJlfL-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sWK_vPKW_2T"
      },
      "outputs": [],
      "source": [
        "!pip install scattertext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1UGJzPuSW_2T"
      },
      "outputs": [],
      "source": [
        "import scattertext as st\n",
        "\n",
        "if \"merge_entities\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
        "\n",
        "if \"merge_noun_chunks\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
        "\n",
        "convention_df = st.SampleCorpora.ConventionData2012.get_data() \n",
        "corpus = st.CorpusFromPandas(convention_df,\n",
        "                             category_col=\"party\",\n",
        "                             text_col=\"text\",\n",
        "                             nlp=nlp).build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2lE7cNO1W_2U"
      },
      "outputs": [],
      "source": [
        "html = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category=\"democrat\",\n",
        "    category_name=\"Democratic\",\n",
        "    not_category_name=\"Republican\",\n",
        "    width_in_pixels=1000,\n",
        "    metadata=convention_df[\"speaker\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true,
        "id": "tf0eHIeaW_2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256f02fa-b6c3-482a-abc6-2eb0f1da4354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "from IPython.core.display import display, HTML\n",
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "print(IN_COLAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZwDPmCJW_2U"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
        "    display(HTML(html))"
      ]
    }
  ]
}