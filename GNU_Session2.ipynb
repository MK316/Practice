{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOY9YvZx4ltSD33f7HBY9E5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkyuhufs/Practice/blob/main/GNU_Session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AI, Digital Literacy, and Convergence Education: Connecting Humanities and Technology\n",
        "\n",
        "##자연어처리와 디지털인문학 따라하기\n",
        "\n",
        "###Junkyu Lee (Hankuk University of Foreign Studies)"
      ],
      "metadata": {
        "id": "jc0IN0eAkiUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B-eitBRrkhgJ"
      },
      "outputs": [],
      "source": [
        "#@markdown Introduction Slides (1~8)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.01.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.03.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.02.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.05.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.04.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.06.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.07.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.08.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 9)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's start with Python\n",
        "##Preprocessing examples"
      ],
      "metadata": {
        "id": "TxTvBlncmnkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Preprocess & Python library Slides (9~11)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.17.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.18.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.11.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 4)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JLYBUfV1rCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Import/Install relevant packages\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Zk3mv9Tdmult"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown \"The rain in Spain falls mainly on the plain.\" 전처리\n",
        "text = \"The rain in Spain falls mainly on the plain.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "    \n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Du694UkntK21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sentence 시각화\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vckRAgRZteiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic modeling w/ 미국 대통령 연설문"
      ],
      "metadata": {
        "id": "id1w71eot1S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown LDA Slides (9~11)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.09.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.10.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 3)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8I-mqeH_mWWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading necessary files\n",
        "\n",
        "* **state-of-the-union.csv:** State of the Union addresses - each presidential address from 1970 to 2012"
      ],
      "metadata": {
        "id": "I_eTrbnruQJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Topic modeling 실습파일](https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv)"
      ],
      "metadata": {
        "id": "QJOIZHBpuRnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Download the csv file\n",
        "# Make data directory if it doesn't exist\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv -P data"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HonmfFlMuV-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Covert csv to Pandas & data cleaning\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/state-of-the-union.csv\")\n",
        "# Clean it up a little bit, removing non-word characters (numbers and ___ etc)\n",
        "df.content = df.content.str.replace(\"[^A-Za-z ]\", \" \")\n",
        "df.head()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DfQCZFFuu3fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Explore data w/ wordcloud\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(df.content.values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "In-xOrwHvSzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data for LDA (Latent Dirichlet Allocation; 잠재 디리클레할당)\n",
        "###gensim.utils.simple_preprocess convert a document into a list of tokens. This lowercases, tokenizes, de-accents (optional)"
      ],
      "metadata": {
        "id": "2JJhthA8NNq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown gensim의 simple_preprocess 이용한 토큰화\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "df.content = df.content.apply(simple_preprocess)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uYBrLYqRNXuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown stopwords 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# stop_words.extend(['from', 'to']) # add more if want\n",
        "df.content = df.content.apply(lambda words: [word for word in words if word not in stop_words])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KKVX4GPQN66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 토큰화 > 카운트벡터 (BOW형태로 변환) 형태인 corpus 생성; 첫번째 결과 확인\n",
        "texts = df.content #Gensim에서는 토큰화된 결과를 texts로 지정해야 함\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5) #출현한 문서 빈도수가 낮거나 (문서에서 5번 이하) or 높은 단어들 (0.5 -> 50%이상) 제외 \n",
        "corpus = [dictionary.doc2bow(text) for text in texts] #doc2bow() >> 토큰화된 결과를 카운트 벡터, 즉 BOW형태로 변환; Gensim에서는 doc2bow()의 결과를 corpus로 지정해야 함\n",
        "corpus[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h_K6oUFUOSTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 카운트 벡터형태의 corpus를 TF-IDF로 변환; 첫 번째 결과 확인\n",
        "from gensim import models\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "corpus_tfidf[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oP-YqoxoO5cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA(잠재 디리클레할당) 실행; 토픽수 = 15개 지정\n",
        "from gensim import models\n",
        "n_topics = 15\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=n_topics)"
      ],
      "metadata": {
        "id": "2LMATQYyPjTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "rUPnohNQP9kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown LDA결과 시각화\n",
        "!pip install pyLDAvis\n",
        "!pip install \"pandas<2.0.0\" \n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "vis"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j1XyigaVQEZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}