{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1HWknktNfG6UJkHZmZPPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkyuhufs/Practice/blob/main/GNU_Session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AI, Digital Literacy, and Convergence Education: Connecting Humanities and Technology\n",
        "\n",
        "##자연어처리와 디지털인문학 따라하기\n",
        "\n",
        "###Junkyu Lee (Hankuk University of Foreign Studies)"
      ],
      "metadata": {
        "id": "jc0IN0eAkiUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B-eitBRrkhgJ"
      },
      "outputs": [],
      "source": [
        "#@markdown Introduction Slides (1~8)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.01.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.03.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.02.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.05.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.04.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.06.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.07.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.08.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 9)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's start with Python\n",
        "##Preprocessing examples"
      ],
      "metadata": {
        "id": "TxTvBlncmnkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Preprocess & Python library Slides (9~11)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.17.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.18.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.11.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 4)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JLYBUfV1rCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Import/Install relevant packages\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Zk3mv9Tdmult"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown \"The rain in Spain falls mainly on the plain.\" 전처리\n",
        "text = \"The rain in Spain falls mainly on the plain.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "    \n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Du694UkntK21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sentence 시각화\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vckRAgRZteiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic modeling w/ 미국 대통령 연설문"
      ],
      "metadata": {
        "id": "id1w71eot1S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown LDA Slides (9~11)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.09.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.10.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 3)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8I-mqeH_mWWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading necessary files\n",
        "\n",
        "* **state-of-the-union.csv:** State of the Union addresses - each presidential address from 1970 to 2012"
      ],
      "metadata": {
        "id": "I_eTrbnruQJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Topic modeling 실습파일](https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv)"
      ],
      "metadata": {
        "id": "QJOIZHBpuRnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Download the csv file\n",
        "# Make data directory if it doesn't exist\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv -P data"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HonmfFlMuV-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Covert csv to Pandas & data cleaning\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/state-of-the-union.csv\")\n",
        "# Clean it up a little bit, removing non-word characters (numbers and ___ etc)\n",
        "df.content = df.content.str.replace(\"[^A-Za-z ]\", \" \")\n",
        "df.head()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DfQCZFFuu3fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Explore data w/ wordcloud\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(df.content.values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "In-xOrwHvSzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data for LDA (Latent Dirichlet Allocation; 잠재 디리클레할당)\n",
        "###gensim.utils.simple_preprocess convert a document into a list of tokens. This lowercases, tokenizes, de-accents (optional)"
      ],
      "metadata": {
        "id": "2JJhthA8NNq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown gensim의 simple_preprocess 이용한 토큰화\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "df.content = df.content.apply(simple_preprocess)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uYBrLYqRNXuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown stopwords 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# stop_words.extend(['from', 'to']) # add more if want\n",
        "df.content = df.content.apply(lambda words: [word for word in words if word not in stop_words])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KKVX4GPQN66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 토큰화 > 카운트벡터 (BOW형태로 변환) 형태인 corpus 생성; 첫번째 결과 확인\n",
        "texts = df.content #Gensim에서는 토큰화된 결과를 texts로 지정해야 함\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5) #출현한 문서 빈도수가 낮거나 (문서에서 5번 이하) or 높은 단어들 (0.5 -> 50%이상) 제외 \n",
        "corpus = [dictionary.doc2bow(text) for text in texts] #doc2bow() >> 토큰화된 결과를 카운트 벡터, 즉 BOW형태로 변환; Gensim에서는 doc2bow()의 결과를 corpus로 지정해야 함\n",
        "corpus[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h_K6oUFUOSTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 카운트 벡터형태의 corpus를 TF-IDF로 변환; 첫 번째 결과 확인\n",
        "from gensim import models\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "corpus_tfidf[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oP-YqoxoO5cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA(잠재 디리클레할당) 실행; 토픽수 = 15개 지정\n",
        "from gensim import models\n",
        "n_topics = 15\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=n_topics)"
      ],
      "metadata": {
        "id": "2LMATQYyPjTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "rUPnohNQP9kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown LDA결과 시각화\n",
        "!pip install pyLDAvis\n",
        "!pip install \"pandas<2.0.0\" \n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "vis"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j1XyigaVQEZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis (w/ Harry Potter)"
      ],
      "metadata": {
        "id": "usEAgkWkRPBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Intro Slides (12~14)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.12.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.13.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.14.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 4)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RNuA9SEtRTft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 분석할 자료 준비"
      ],
      "metadata": {
        "id": "OxA50awOT-Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Harry Potter 자료를 공유한 깃허브 사이트](https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git)"
      ],
      "metadata": {
        "id": "6zTPhIAnRgx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Harry Potter자료 가져오기\n",
        "!git clone https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jzAN3jZORiE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Pandas이용 데이터 정리; 책의 한 챕터가 한 셀에 있는 상태\n",
        "import pandas as pd #Importing Pandas package\n",
        "%cd /content/Harry-Potter-Text-Mining/Book Text\n",
        "\n",
        "import glob \n",
        "fns = glob.glob('*.txt')\n",
        "df = pd.DataFrame()\n",
        "for fn in fns:\n",
        "  dftmp = pd.read_csv(fn, sep=\"@\")\n",
        "  df = pd.concat([df, dftmp])\n",
        "\n",
        "%cd /content\n",
        "\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wd11Hx9_R0EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 데이터 전처리 Stopwords제거\n",
        "import nltk #Import NLTK library\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') #installed punkt to fix error\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords #Import stopwords to Python\n",
        "\n",
        "stopwords = set(stopwords.words('english')) #English stopwords assigned to \"stopwords\" object\n",
        "\n",
        "import string #Punctuation\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "stopwords = [''.join(item for item in x if item not in string.punctuation) for x in stopwords] #Remove punctuation from stopwords\n",
        "\n",
        "df['WordCountText']=df['Text'].str.lower().apply(remove_punctuations).apply(word_tokenize) # Word Count Text\n",
        "# Word Count\n",
        "df['WordCloudText']=df['WordCountText'].apply(lambda x: [word for word in x if word not in stopwords]) # Word Cloud Text\n",
        "df['WordCount'] = df['WordCountText'].str.len() #Word Count Per Chapter"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gOMavAe8SOyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 데이터 전처리: 책 > 문장단위로 (챕터가 문장단위로 나뉜 상태)\n",
        "# Creating a table breaking down the text by each sentence, rather than each chapter.\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Make smaller table - reset index to prepare for further work\n",
        "dfsentiment = df[['Book','Chapter','Text']].reset_index() \\\n",
        "    .drop([\"index\"], axis=1)\n",
        "dfsentiment = dfsentiment.join(dfsentiment.Text.apply(sent_tokenize).rename('Sentences')) # Breaking apart text into sentences\n",
        "\n",
        "#Put every tokenized sentence into its own row\n",
        "dfsentiment2 = dfsentiment.Sentences.apply(pd.Series) \\\n",
        "    .merge(dfsentiment, left_index = True, right_index = True) \\\n",
        "    .drop([\"Text\"], axis = 1) \\\n",
        "    .drop([\"Sentences\"], axis = 1) \\\n",
        "    .melt(id_vars = ['Book', 'Chapter'], value_name = \"Sentence\") \\\n",
        "    .drop(\"variable\", axis = 1) \\\n",
        "    .dropna()\n",
        "\n",
        "# Sort new table by Book and Chapter - reset index to reflect new order\n",
        "dfsentiment2=dfsentiment2.sort_values(by=['Book', 'Chapter']) \\\n",
        "    .reset_index() \\\n",
        "    .drop(['index'], axis = 1)\n",
        "\n",
        "# Clean punctuation, lower case\n",
        "dfsentiment2['Sentence']=dfsentiment2.Sentence.apply(remove_punctuations).apply(lambda x: x.lower()) \\\n",
        "\n",
        "# Check first five values\n",
        "dfsentiment2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zcC4zJ9JSkso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##감정분석 실행"
      ],
      "metadata": {
        "id": "T-ATz9QaUDfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Bring in VADER library & Sentiment Intensity Analyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "sid=nltk.sentiment.vader.SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hF38sFIGUHdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 문장별로 감정분석 점수 부여; Compound, positive, negative, neutral \n",
        "# Get intensity scores of each sentence\n",
        "dfsentiment2['Score']=dfsentiment2.Sentence.apply(lambda x: sid.polarity_scores(x))\n",
        "\n",
        "# Place scores in own columns\n",
        "dfsentiment2['CompScore']=dfsentiment2.Score.apply(lambda x: x.get(\"compound\"))\n",
        "dfsentiment2['PosScore']=dfsentiment2.Score.apply(lambda x: x.get(\"pos\"))\n",
        "dfsentiment2['NegScore']=dfsentiment2.Score.apply(lambda x: x.get(\"neg\"))\n",
        "dfsentiment2['NeuScore']=dfsentiment2.Score.apply(lambda x: x.get(\"neu\"))\n",
        "\n",
        "# With scores extracted, the original score field can be removed\n",
        "dfsentiment2 = dfsentiment2.drop([\"Score\"], axis=1)\n",
        "\n",
        "# Adding Sentiment Flags\n",
        "dfsentiment2['PosFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x >= 0.05 else 0)\n",
        "dfsentiment2['NegFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x <= -0.05 else 0)\n",
        "dfsentiment2['NeuFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x < 0.05 and x > -0.05 else 0)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9wIjPpC3Uj6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfsentiment2.head(20)"
      ],
      "metadata": {
        "id": "QHL5QRuwU_f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfsentiment2.groupby('Book').mean()['CompScore']"
      ],
      "metadata": {
        "id": "pcloYnouWrmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 긍정, 부정, 중립의 문장 수 확인\n",
        "print('There are', dfsentiment2['PosFlag'].sum(), \"positive sentences,\", \\\n",
        "      dfsentiment2['NegFlag'].sum(), \"negative sentences, and\", \\\n",
        "      dfsentiment2['NeuFlag'].sum(), \"neutral sentences\")\n",
        "print('This is a total of', \\\n",
        "      dfsentiment2['PosFlag'].sum()+dfsentiment2['NegFlag'].sum()+dfsentiment2['NeuFlag'].sum(), \"sentences\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kChwC_SfVY-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###visualization of sentiment analysis"
      ],
      "metadata": {
        "id": "nz5KP9vxV5m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown How does the sentiment change during each chapter of each book over time?\n",
        "#Time series of sentiments in 7 books of Harry Potter\n",
        "def Titles(x):\n",
        "    if x == 1:\n",
        "        return \"1 - Sorcerer's Stone\"\n",
        "    if x == 2:\n",
        "        return \"2 - Chamber of Secrets\"\n",
        "    if x == 3:\n",
        "        return \"3 - Prizoner of Azkaban\"\n",
        "    if x == 4:\n",
        "        return \"4 - Goblet of Fire\"\n",
        "    if x == 5:\n",
        "        return \"5 - Order of the Phoenix\"\n",
        "    if x == 6:\n",
        "        return \"6 - Half Blood Prince\"\n",
        "    if x == 7:\n",
        "        return \"7 - Deathly Hallows\"\n",
        "\n",
        "dfsentiment2['BookTitle']=dfsentiment2.Book.apply(lambda x: Titles(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "colorsList = ['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'] #Mauraders Map Colors\n",
        "ColorMap = matplotlib.colors.ListedColormap(colorsList)\n",
        "\n",
        "# plot data\n",
        "fig, ax = plt.subplots(figsize=(15,13))\n",
        "# use unstack()\n",
        "dfsentiment2.groupby(['Chapter','BookTitle']).mean()['CompScore'].unstack().plot(ax=ax, subplots=True, ylim=(-0.25, 0.25), colormap=ColorMap)\n",
        "plt.style.use('ggplot')\n",
        "ax.set_ylabel('Compound Sentiment Score')\n",
        "\n",
        "[ax.legend(loc=1) for ax in plt.gcf().axes]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pcdnCk0cWACI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cluster analysis w/ movie reviews"
      ],
      "metadata": {
        "id": "5PeqhNiQX2F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown CA intro Slides (15~16)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.15.JPG\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/Practice/main/slide.16.JPG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 3)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"700\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4GDHpqLlX6Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Import/Install relevant packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "from sklearn import feature_extraction\n",
        "!pip install mpld3\n",
        "import mpld3\n",
        "import requests"
      ],
      "metadata": {
        "cellView": "form",
        "id": "krYuonNxYGQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##movie reviews자료 준비하기"
      ],
      "metadata": {
        "id": "viUftP9bYoKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[titles](https://raw.githubusercontent.com/brandomr/document_cluster/master/title_list.txt)"
      ],
      "metadata": {
        "id": "A40TRD8fYtGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[genres](https://raw.githubusercontent.com/brandomr/document_cluster/master/genres_list.txt)"
      ],
      "metadata": {
        "id": "y-auGSkWYymH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[synopses(wiki)](https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_wiki.txt)"
      ],
      "metadata": {
        "id": "MPcVWX1yY2q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[synopses(imdb)](https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_imdb.txt)"
      ],
      "metadata": {
        "id": "PPDVMapCY742"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 사이트에서 상위 100개의 자료를 각각 가져오기\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/title_list.txt\"\n",
        "titles = requests.get(url).text.split('\\n')\n",
        "titles = titles[:100]\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/genres_list.txt\"\n",
        "genres = requests.get(url).text.split('\\n')\n",
        "genres = genres[:100]\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_wiki.txt\"\n",
        "synopses_wiki = requests.get(url).text.split('\\n BREAKS HERE')\n",
        "synopses_wiki = synopses_wiki[:100]\n",
        "# cleaning\n",
        "synopses_clean_wiki = []\n",
        "for text in synopses_wiki:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_wiki.append(text)\n",
        "synopses_wiki = synopses_clean_wiki\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_imdb.txt\"\n",
        "synopses_imdb = requests.get(url).text.split('\\n BREAKS HERE')\n",
        "synopses_imdb = synopses_imdb[:100]\n",
        "# cleaning\n",
        "synopses_clean_imdb = []\n",
        "for text in synopses_imdb:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_imdb.append(text)\n",
        "synopses_imdb = synopses_clean_imdb"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sydRxvLIZBrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown wiki와 imdb의 synopsis 합치기\n",
        "synopses = []\n",
        "for i in range(len(synopses_wiki)):\n",
        "    item = synopses_wiki[i] + synopses_imdb[i]\n",
        "    synopses.append(item)\n",
        "synopses[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fg2-9hawZmMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 영화순위 저장\n",
        "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
        "ranks = []\n",
        "for i in range(0,len(titles)):\n",
        "    ranks.append(i)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vDyb-eN8aToq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 영화제목, 줄거리, 장르, 순위가 몇 개씩 들어왔는지 확인\n",
        "print(str(len(titles)) + ' titles')\n",
        "print(str(len(synopses)) + ' synopses')\n",
        "print(str(len(genres)) + ' genres')\n",
        "print(str(len(ranks)) + ' ranks')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "I2hBAE7KabXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data cleaning"
      ],
      "metadata": {
        "id": "KRGSxB0aa4u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown NLTK에서 stopwords와 stemmer 가져오기\n",
        "# load nltk's English stopwords as variable called 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "J0OmNHgEa9RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown tokenize with stemming위한 함수 만들기\n",
        "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AWCJ_7dLbsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Stemming한 함수적용하여 단어리스트 생성 및 확인\n",
        "nltk.download('punkt')\n",
        "totalvocab_stemmed = []\n",
        "for i in synopses:\n",
        "    allwords_stemmed = tokenize_and_stem(i)\n",
        "    totalvocab_stemmed.extend(allwords_stemmed)\n",
        "\n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
        "vocab_frame"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CqtBqdHXb3IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means clustering"
      ],
      "metadata": {
        "id": "oogVU0uuf9Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown TF-IDF이용 벡터로 변환; 빈도가 너무 많거나 적은 것은 배제\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0NPtUctyf0L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown TF-IDF에 이용된 단어리스트 확인\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "terms"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ntf4sEDXhjFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 코사인 유사도를 이용한 거리 계산\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = 1 - cosine_similarity(tfidf_matrix)\n",
        "dist"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_0yfOgF7h3uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist.shape"
      ],
      "metadata": {
        "id": "10Hlu4o4iDnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 군집을 5개로 정하여 군집분석\n",
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 5\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "clusters = km.labels_.tolist()\n",
        "clusters"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-ZiRjI5GjLhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
        "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
        "frame"
      ],
      "metadata": {
        "id": "98QtRNy7jfxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame['cluster'].value_counts()"
      ],
      "metadata": {
        "id": "YCK7BeJEj4l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = frame['rank'].groupby(frame['cluster'])\n",
        "grouped.mean()"
      ],
      "metadata": {
        "id": "NnjImUKHkCn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 각 군집별 주요단어와 영화목록 확인\n",
        "print(\"Top terms per cluster:\")\n",
        "print()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    for ind in order_centroids[i, :6]:\n",
        "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Cluster %d titles:\" % i, end='')\n",
        "    for title in frame.loc[i]['title'].values.tolist():\n",
        "        print(' %s,' % title, end='')\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lTXZUaGrkGKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multidimensional scaling (시각화를 위한 차원 축소 563 -> 2)"
      ],
      "metadata": {
        "id": "lH_TQB26k-aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 관련 package불러온 후 MDS실행\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
        "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
        "xs, ys = pos[:, 0], pos[:, 1]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rsVjrXcVk_SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 군집의 색깔과 이름 지정\n",
        "#set up colors per clusters using a dict\n",
        "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
        "\n",
        "#set up cluster names using a dict\n",
        "cluster_names = {0: 'Family, home, war', \n",
        "                 1: 'Police, killed, murders', \n",
        "                 2: 'Father, New York, brothers', \n",
        "                 3: 'Dance, singing, love', \n",
        "                 4: 'Killed, soldiers, captain'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "72PfJbFFlbY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown MDS시각화\n",
        "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')\n",
        "\n",
        "# set up plot\n",
        "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
        "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "#iterate through groups to layer the plot\n",
        "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
        "for name, group in groups:\n",
        "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
        "    ax.set_aspect('auto')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelbottom='off')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'y',         # changes apply to the y-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        left='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelleft='off')\n",
        "    \n",
        "ax.legend(numpoints=1)  #show legend with only 1 point\n",
        "\n",
        "#add label in x,y position with the label as the film title\n",
        "for i in range(len(df)):\n",
        "    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)\n",
        "    \n",
        "plt.show() #show the plot"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NFHB9RrMlsdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hierarchical document clustering(dendrogram)"
      ],
      "metadata": {
        "id": "7TZ7VFAEl7Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Dendrogram을 이용한 시각화\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
        "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
        "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
        "plt.tick_params(\\\n",
        "    axis= 'x',          # changes apply to the x-axis\n",
        "    which='both',      # both major and minor ticks are affected\n",
        "    bottom='off',      # ticks along the bottom edge are off\n",
        "    top='off',         # ticks along the top edge are off\n",
        "    labelbottom='off')\n",
        "plt.tight_layout() #show plot with tight layout"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E_1nJZfwl9Ek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}