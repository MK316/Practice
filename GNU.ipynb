{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNu1fEpQ8g23sUW6CJvAvpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkyuhufs/Practice/blob/main/GNU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic modeling"
      ],
      "metadata": {
        "id": "szexyGDwLttR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading necessary files\n",
        "\n",
        "* **state-of-the-union.csv:** State of the Union addresses - each presidential address from 1970 to 2012"
      ],
      "metadata": {
        "id": "crzKNK3zL0uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Topic modeling 실습파일](https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv)"
      ],
      "metadata": {
        "id": "TYMqLdplMOTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bck2ExwDLqKw"
      },
      "outputs": [],
      "source": [
        "# Make data directory if it doesn't exist\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv -P data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data cleaning"
      ],
      "metadata": {
        "id": "rR-f7znWNB5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/state-of-the-union.csv\")\n",
        "# Clean it up a little bit, removing non-word characters (numbers and ___ etc)\n",
        "df.content = df.content.str.replace(\"[^A-Za-z ]\", \" \")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Q3iQXNEvNCxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore data with Wordcloud"
      ],
      "metadata": {
        "id": "JSOe7hi7NVNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(df.content.values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "6xR2jQMANWKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data for LDA (Latent Dirichlet Allocation; 잠재 디리클레할당)"
      ],
      "metadata": {
        "id": "sfg_fXesNqkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gensim.utils.simple_preprocess convert a document into a list of tokens. This lowercases, tokenizes, de-accents (optional)"
      ],
      "metadata": {
        "id": "1RDYfyBxOAi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "0ZopNCdEO12F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "df.content = df.content.apply(simple_preprocess)"
      ],
      "metadata": {
        "id": "3fwSaKJxN6F8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove stopwords"
      ],
      "metadata": {
        "id": "N9jNyq4gPemt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# stop_words.extend(['from', 'to']) # add more if want\n",
        "df.content = df.content.apply(lambda words: [word for word in words if word not in stop_words])"
      ],
      "metadata": {
        "id": "RmoYnIE-PcSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using LDA with Gensim"
      ],
      "metadata": {
        "id": "6nx8K6lDPp_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df.content\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "corpus[0]"
      ],
      "metadata": {
        "id": "fUMiQt2VPq8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "corpus_tfidf[0]"
      ],
      "metadata": {
        "id": "ldN1gfJiP60y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "n_topics = 15\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=n_topics)"
      ],
      "metadata": {
        "id": "CO5Vr4yCQChW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "uAXN6RaKQISD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install \"pandas<2.0.0\" \n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "7n59ZsHJQS7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis"
      ],
      "metadata": {
        "id": "VVQS-5sjQuPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis (w/ Harry Potter)"
      ],
      "metadata": {
        "id": "d8OKrBT7Ujd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load** data"
      ],
      "metadata": {
        "id": "8gBPLoAcUx84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Harry Potter 자료를 공유한 깃허브 사이트](https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git)"
      ],
      "metadata": {
        "id": "TbpQ_c83VF4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git"
      ],
      "metadata": {
        "id": "udNwYED0UxN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #Importing Pandas package\n",
        "%cd /content/Harry-Potter-Text-Mining/Book Text\n",
        "\n",
        "import glob \n",
        "fns = glob.glob('*.txt')\n",
        "df = pd.DataFrame()\n",
        "for fn in fns:\n",
        "  dftmp = pd.read_csv(fn, sep=\"@\")\n",
        "  df = pd.concat([df, dftmp])\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "-rpW9r_KUsA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "oxQ1jqM3Vykc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing data"
      ],
      "metadata": {
        "id": "Or0sbHltWCgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #Import NLTK library\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') #installed punkt to fix error\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords #Import stopwords to Python\n",
        "\n",
        "stopwords = set(stopwords.words('english')) #English stopwords assigned to \"stopwords\" object\n",
        "\n",
        "import string #Punctuation\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "stopwords = [''.join(item for item in x if item not in string.punctuation) for x in stopwords] #Remove punctuation from stopwords\n",
        "\n",
        "df['WordCountText']=df['Text'].str.lower().apply(remove_punctuations).apply(word_tokenize) # Word Count Text\n",
        "# Word Count\n",
        "df['WordCloudText']=df['WordCountText'].apply(lambda x: [word for word in x if word not in stopwords]) # Word Cloud Text\n",
        "df['WordCount'] = df['WordCountText'].str.len() #Word Count Per Chapter"
      ],
      "metadata": {
        "id": "of75yrXiWDqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a table breaking down the text by each sentence, rather than each chapter.\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Make smaller table - reset index to prepare for further work\n",
        "dfsentiment = df[['Book','Chapter','Text']].reset_index() \\\n",
        "    .drop([\"index\"], axis=1)\n",
        "dfsentiment = dfsentiment.join(dfsentiment.Text.apply(sent_tokenize).rename('Sentences')) # Breaking apart text into sentences\n",
        "\n",
        "#Put every tokenized sentence into its own row\n",
        "dfsentiment2 = dfsentiment.Sentences.apply(pd.Series) \\\n",
        "    .merge(dfsentiment, left_index = True, right_index = True) \\\n",
        "    .drop([\"Text\"], axis = 1) \\\n",
        "    .drop([\"Sentences\"], axis = 1) \\\n",
        "    .melt(id_vars = ['Book', 'Chapter'], value_name = \"Sentence\") \\\n",
        "    .drop(\"variable\", axis = 1) \\\n",
        "    .dropna()\n",
        "\n",
        "# Sort new table by Book and Chapter - reset index to reflect new order\n",
        "dfsentiment2=dfsentiment2.sort_values(by=['Book', 'Chapter']) \\\n",
        "    .reset_index() \\\n",
        "    .drop(['index'], axis = 1)\n",
        "\n",
        "# Clean punctuation, lower case\n",
        "dfsentiment2['Sentence']=dfsentiment2.Sentence.apply(remove_punctuations).apply(lambda x: x.lower()) \\\n",
        "\n",
        "# Check first five values\n",
        "dfsentiment2"
      ],
      "metadata": {
        "id": "sTMIMgqoWfXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 71,016 rows in sentiment2 according to the table created\n",
        "# Check that there are 71,016 sentences before stacking rows\n",
        "dfsentiment.Sentences.apply(lambda x: len(x)).sum() # Count total sentences"
      ],
      "metadata": {
        "id": "Bxfz1JmGWpiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment analysis"
      ],
      "metadata": {
        "id": "VVXZ2vGOW11I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring in VADER library and Sentiment Intensity Analyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "sid=nltk.sentiment.vader.SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "c3IZMXGxW4MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get intensity scores of each sentence\n",
        "dfsentiment2['Score']=dfsentiment2.Sentence.apply(lambda x: sid.polarity_scores(x))\n",
        "\n",
        "# Place scores in own columns\n",
        "dfsentiment2['CompScore']=dfsentiment2.Score.apply(lambda x: x.get(\"compound\"))\n",
        "dfsentiment2['PosScore']=dfsentiment2.Score.apply(lambda x: x.get(\"pos\"))\n",
        "dfsentiment2['NegScore']=dfsentiment2.Score.apply(lambda x: x.get(\"neg\"))\n",
        "dfsentiment2['NeuScore']=dfsentiment2.Score.apply(lambda x: x.get(\"neu\"))\n",
        "\n",
        "# With scores extracted, the original score field can be removed\n",
        "dfsentiment2 = dfsentiment2.drop([\"Score\"], axis=1)\n",
        "\n",
        "# Adding Sentiment Flags\n",
        "dfsentiment2['PosFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x >= 0.05 else 0)\n",
        "dfsentiment2['NegFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x <= -0.05 else 0)\n",
        "dfsentiment2['NeuFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x < 0.05 and x > -0.05 else 0)"
      ],
      "metadata": {
        "id": "vxZzZC4RXGrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm scores are present and flag is functional\n",
        "dfsentiment2.head(20)"
      ],
      "metadata": {
        "id": "UJ1A4HCUXQNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfsentiment2.groupby('Book').mean()['CompScore']"
      ],
      "metadata": {
        "id": "uajozJaHXgzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('There are', dfsentiment2['PosFlag'].sum(), \"positive sentences,\", \\\n",
        "      dfsentiment2['NegFlag'].sum(), \"negative sentences, and\", \\\n",
        "      dfsentiment2['NeuFlag'].sum(), \"neutral sentences\")\n",
        "print('This is a total of', \\\n",
        "      dfsentiment2['PosFlag'].sum()+dfsentiment2['NegFlag'].sum()+dfsentiment2['NeuFlag'].sum(), \"sentences\")"
      ],
      "metadata": {
        "id": "-omOSOudXkd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualization of sentiment analysis"
      ],
      "metadata": {
        "id": "OjgH053rX1Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the sentiment change during each chapter of each book over time?\n",
        "def Titles(x):\n",
        "    if x == 1:\n",
        "        return \"1 - Sorcerer's Stone\"\n",
        "    if x == 2:\n",
        "        return \"2 - Chamber of Secrets\"\n",
        "    if x == 3:\n",
        "        return \"3 - Prizoner of Azkaban\"\n",
        "    if x == 4:\n",
        "        return \"4 - Goblet of Fire\"\n",
        "    if x == 5:\n",
        "        return \"5 - Order of the Phoenix\"\n",
        "    if x == 6:\n",
        "        return \"6 - Half Blood Prince\"\n",
        "    if x == 7:\n",
        "        return \"7 - Deathly Hallows\"\n",
        "\n",
        "dfsentiment2['BookTitle']=dfsentiment2.Book.apply(lambda x: Titles(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "colorsList = ['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'] #Mauraders Map Colors\n",
        "ColorMap = matplotlib.colors.ListedColormap(colorsList)\n",
        "\n",
        "# plot data\n",
        "fig, ax = plt.subplots(figsize=(15,13))\n",
        "# use unstack()\n",
        "dfsentiment2.groupby(['Chapter','BookTitle']).mean()['CompScore'].unstack().plot(ax=ax, subplots=True, ylim=(-0.25, 0.25), colormap=ColorMap)\n",
        "plt.style.use('ggplot')\n",
        "ax.set_ylabel('Compound Sentiment Score')\n",
        "\n",
        "[ax.legend(loc=1) for ax in plt.gcf().axes]"
      ],
      "metadata": {
        "id": "I06N9AAzX5Vj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}